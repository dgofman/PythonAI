# -*- coding: utf-8 -*-
"""GradientDescent_homework1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YlXenKvxsq85j2RiY_iGxUw-fs_lZJPy

# Implementing the Gradient Descent Algorithm

In this lab, we'll implement the basic functions of the Gradient Descent algorithm to find the boundary in a small dataset. First, we'll start with some functions that will help us plot and visualize the data.
"""

#import urllib.request
#urllib.request.urlretrieve('https://raw.githubusercontent.com/dgofman/PythonAI/main/Deep%20Learning/Homework2/data.csv', '/content/data.csv')

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np
import pandas as pd

#Some helper functions for plotting and drawing lines

plt.figure(figsize=(15, 10))

def plot_points(X, y):
    admitted = X[np.argwhere(y==1)]
    rejected = X[np.argwhere(y==0)]
    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')
    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')

def display(m, b, color, alpha=1):
    plt.xlim(-0.05,1.05)
    plt.ylim(-0.05,1.05)
    x = np.arange(-10, 10, 0.1)
    plt.plot(x, m*x+b, color, alpha=alpha)

"""## Reading and plotting the data"""

data = pd.read_csv('data.csv', header=None)
X = np.array(data[[0,1]])
y = np.array(data[2])
plot_points(X,y)
#plt.show()

"""## Implementing the basic functions
Here is your turn to shine. Implement the following formulas, as explained in the text.
Sigmoid activation function
    ðœŽ(ð‘¥)=1/(1+ð‘’âˆ’ð‘¥)

Output (prediction) formula
    ð‘¦Ì‚ =ðœŽ(ð‘¤1ð‘¥1+ð‘¤2ð‘¥2+ð‘)

Error function
    ð¸ð‘Ÿð‘Ÿð‘œð‘Ÿ(ð‘¦,ð‘¦Ì‚ )=âˆ’ð‘¦log(ð‘¦Ì‚ )âˆ’(1âˆ’ð‘¦)log(1âˆ’ð‘¦Ì‚ )

The function that updates the weights
    ð‘¤ð‘–âŸ¶ð‘¤ð‘–+ð›¼(ð‘¦âˆ’ð‘¦Ì‚ )ð‘¥ð‘–
    ð‘âŸ¶ð‘+ð›¼(ð‘¦âˆ’ð‘¦Ì‚ )
"""

# Implement the following functions

# Activation (sigmoid) function
def sigmoid(x):
    # sigma(ð‘¥) = 1 / (1 + ð‘’âˆ’ð‘¥)
    return 1 / (1 + np.exp(-x)) # https://www.digitalocean.com/community/tutorials/sigmoid-activation-function-python
    
# Output (prediction) formula
def output_formula(features, weights, bias):
    # ð‘¤1ð‘¥1 + ð‘¤2ð‘¥2
    if len(features) == 2: # 1-D NumPy Arrays (train -> update_weights -> output_formula)
        sum = features[0] * weights[0] + features[1] * weights[1] #https://sparkbyexamples.com/numpy/numpy-dot-function/
    else:
        sum = np.dot(features, weights) #(train -> output_formula)
    # y_hat = sigma(ð‘¤1ð‘¥1 + ð‘¤2ð‘¥2 + ð‘)
    return sigmoid(sum + bias)

# Error (log-loss) formula
def error_formula(y, output):
    y_hat = np.float64(output) # = y^
    #Error(y, y^) = âˆ’ylogâ¡(y^) âˆ’ (1 âˆ’ y) * logâ¡(1âˆ’y^)
    return -y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)

# Gradient descent step
def update_weights(x, y, weights, bias, learnrate):
    # Î±(y âˆ’ y^)
    Î± = (y - output_formula(x, weights, bias)) * learnrate
    # wiâŸ¶wi + Î±(y âˆ’ y^) x
    weights = weights + Î± * x
    # bâŸ¶b + Î±(y âˆ’ y^)
    bias = bias + Î±
    return weights, bias

"""## Training function
This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm.
"""

np.random.seed(44)

epochs = 3000
learnrate = 0.01

def train(features, targets, epochs, learnrate):
    
    errors = []
    _, n_features = features.shape
    last_loss = None
    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)
    best_weights = []
    best_epochs = 0
    best_accuracy = 0
    best_bias = 0
    bias = 0
    for e in range(epochs):
        #del_w = np.zeros(weights.shape)
        for x, y in zip(features, targets):
        #    output = output_formula(x, weights, bias)
        #    error = error_formula(y, output)
            weights, bias = update_weights(x, y, weights, bias, learnrate)
        
        # Printing out the log-loss error on the training set
        out = output_formula(features, weights, bias)
        loss = np.mean(error_formula(targets, out))
        errors.append(loss)

        predictions = out > 0.5
        accuracy = np.mean(predictions == targets)
        #print("Accuracy: ", accuracy)

        if accuracy >= best_accuracy:
            best_accuracy = accuracy
            best_weights = weights
            best_bias = bias
            best_epochs = e
            
        if e == 0:
            display(-weights[0]/weights[1], -bias/weights[1], 'red')
        elif last_loss - loss > 0.001:
            display(-weights[0]/weights[1], -bias/weights[1], 'green', alpha=.2)

        last_loss = loss
            

    # Plotting the solution boundary
    plt.title("Iterations: {} / {}, Last Accuracy: {}, Best Accuracy: {}".format(epochs, best_epochs, accuracy, best_accuracy))

    print("Last Accuracy: ", accuracy)
    display(-weights[0]/weights[1], -bias/weights[1], 'black')

    print("Best Accuracy: ", best_accuracy)
    display(-best_weights[0]/best_weights[1], -best_bias/best_weights[1], 'magenta')

    # Plotting the data
    plt.legend(handles=[
        mpatches.Patch(color='red', label='Start Line'),
        mpatches.Patch(color='green', label='Last Loss Line'),
        mpatches.Patch(color='black', label='Last Accuracy Line'),
        mpatches.Patch(color='magenta', label='Best Accuracy Line')
    ])
    plot_points(features, targets)
    plt.show()


"""## Time to train the algorithm!
When we run the function, we'll obtain the following:
- 10 updates with the current training loss and accuracy
- A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.
- A plot of the error function. Notice how it decreases as we go through more epochs.
"""

train(X, y, epochs, learnrate)